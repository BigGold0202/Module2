{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Suggestions for merchants and rate prediction based on Yelp dataset </h1>\n",
    "\n",
    "  \n",
    "## 1. Introduction\n",
    "### 1.1 Motivation and thesis statement\n",
    "Yelp is a popular software in people's daily life. For merchants, yelp reviews play an important role in business operations. We can obtain information about user experience and sentiment by analyzing the text of yelp reviews. So the goal of this project is to provide useful, analytical insights to business owners on Yelp and, based on these insights, make actionable suggestions to owners in order to improve their ratings in Yelp. We also propose a prediction model to predict the ratings of reviews based on the text and related attributes. \n",
    "### 1.2 Background information about data\n",
    "For review data, the training dataset is based on 5,364,626 reviews. It contains business_id, date, stars and text. In the test and validation data, there are 1,321,274 elements without rating. For the business data, there are 154,606 businesses in the training dataset, which contains specific information about businesses' attributes, business_id, categories, city, hours, is_open, latitude, longitude, name, postal_code, state, and 38,000 observations in the test dataset.\n",
    "## 2. Data pre-processing\n",
    "### 2.1 Data cleaning for the text of review data\n",
    "(1) Convert all uppercase letters to lowercase. <br>\n",
    "(2) Replace all commas with periods, since we use a period to seperate a relatively emotionally independent statement in the following analysis.<br>\n",
    "(3) Convert words with negative meaning, e.g., n'/n't $\\to$ not.<br>\n",
    "(4) Keep special symbols including ! and combination of !, ., ..., !?, :).<br>\n",
    "(5) Delete stopwords based on nltk[1] stopword corpus except: i) words with negative meaning: not, no, nor; ii) words that represents a third person: he, him, his, himself, she, her, she's, her, hers, herself, they, them, their, theirs, themselves.<br>\n",
    "(6) Lemmatization: use a dictionary to return the original word of verb and noun, e.g., ran $\\to$ run, timing $\\to$ time.<br>\n",
    "(7) Negation: add _NEG to each word between the negation and the following first period.<br>\n",
    "\n",
    "### 2.2 Dataset selection\n",
    "We filter the business_id of the \"categories\" that contains \"Brunch\" in the business data and correspond it to whole review data. Then we obtained the brunch review data which includes 521673 observations of 4322 businesses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Goal 2: Prediction\n",
    "\n",
    "We use all the train reviews to build a multinomial logistics regression with TF-IDF model with 5 results (1-5 star), and the final RMSE of the test set is 0.81245.\n",
    "\n",
    "### 4.1 Bag of words\n",
    "The bag of words model assumes that we do not consider the context between words and words in the texts, only the weights of all words. The weight is related to frequency of each word appears in the texst.\n",
    "\n",
    "First, we tokenize the text into words. After tokenization, by counting the frequency each word appears in the text and extracting feature words by Chi-square test, we can get the word-based feature of the texts. Placing the words of each text together with the corresponding word frequency is the vectorization. After the vectorization, we use TF-IDF to perform weight correction of the features, and then normalize TF-IDF. After performing these feature extraction, we apply the training data to the machine learning model.\n",
    "\n",
    "### 4.2 CountVectorizer/ TF-IDF\n",
    "\n",
    "CountVectorizer[2] converts the words in the texts into a word frequency matrix. For example, the matrix contains an element $a[i, j]$, which represents the word frequency of the _j word under the i-th_ text. \n",
    "\n",
    "TF-IDF tends to filter out common words and retain important words. \n",
    "\n",
    "Let $w$, $t$ denote a word and a text, TF-IDF (Term Frequency $\\times$ Inverse Document Frequency) is calculated by \n",
    "$$\\text{TF}(w,t) = \\cfrac{\\# w\\ \\text{in}\\ t}{\\text{#words in}\\ t}, \\quad \\text{IDF}(w,t) = \\log\\cfrac{\\text{#words in}\\ t}{\\text{#texts that contain}\\ w}, \\quad \\text{TF-IDF} = \\text{TF} \\times \\text{IDF}$$\n",
    "\n",
    "Then we apply normalization to the TF-IDF algorithm. Since high TF-IDF of some words may correspond to higher similarity between texts, normalization helps us interpret the similarity score better from the tf-idf score, it is also useful to utilize the tf-idf score as a feature in our classifier model. \n",
    "\n",
    "### 4.3 Variable selection\n",
    "\n",
    "In our project, at first we screen out high frequency words in test and validation data by CountVectorizer, since we think the low frequency words, even if they are important, will only work for very small amounts of data, not make a big difference to the whole. Next, we correspond the high frequency words selected from the test and validation data to the training data, and use them to conduct variable selection in the training data based on Chi-square test. Based on the value of chi-square statistic, we choose the top 2000 significant words of training data to build wordlist. Similarly, we also use the feature importance score of random forest, and select 2000 significant words.\n",
    "\n",
    "\n",
    "### 4.4 Model fitting\n",
    "\n",
    "For the cleaned training data, we apply the sparse matrix whose columns represent words values TFIDF to logistics regression. \n",
    "\n",
    "We use multinomial Logistic regression with optional L2 regularization in scikit-learn.[3] For the model parameters, we choose 'multinomial' over 'one vs rest' for better accuracy, normalize the data and use Stochastic Average Gradient solver for faster convergence speed and better robustness. As for regularization strength C, we use prediction accuracy with grid search and 5 fold cross validation to set it to 1. \n",
    "\n",
    "### 4.5 Model evaluation\n",
    "\n",
    "We use the learning curve to verify the robustness of the model. From the learning curve plot, we could see the score is approximately converges to 0.682, which proves the low bias property. With the increase of training sample set, the train score decreases and cross-validation score increases and come closer, which proves the low variance. These indicates the Logistic regression model is robust and doesn't suffer from overfitting or underfitting. \n",
    "![Image](https://github.com/BigGold0202/Module2/blob/master/image/learning%20curve.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Conclusion\n",
    "### 5.1 Suggestions\n",
    "\n",
    "For the business attributes, \n",
    "Most of the results are quite intuitive: \n",
    "Beer and wine is the best option for alcohol, even better than a full bar.\n",
    "The restaurants which are able to cater tend to have better reviews.\n",
    "The restaurants have TVs, free Wifi, out door seating options and wheel chair accessiblilty tend to have better reviews.\n",
    "And expensive restaurants and quite restaurants tend to have better reviews.\n",
    "\n",
    "Serveral results need some assumptions to understand:\n",
    "The restaurants which don't accept credit cards tend to have better reviews. We suppose it's because these are very small restaurants run by individuals. They need to provide really good food to attract people.\n",
    "\n",
    "A brunch restaurant that supplies cocktail usually have a higher rate.\n",
    "Multiple selections of alcohol and a cocktail menu usually means a higher rate.\n",
    "If appetizer is available, shrimp cocktail is a good selection.\n",
    "Special cocktail sauce and signature cocktail are essential conditions for a good brunch in a cocktail business.\n",
    "\n",
    "\n",
    "### 5.2 Strength and weakness\n",
    "\n",
    "Strengths: 1. We use both business attributes and keywords in the reviews to make suggestion, it is feasible and sufficient. 2. Through tuning parameters, our prediction model has been improved in robustness.\n",
    "\n",
    "\n",
    "Weakness: 1. We need We need to interpret the code results manually, which is not completely automatic. 2. We should fit more models for rating prediction and compare them.\n",
    "\n",
    "\n",
    "### 5.3 Contributions\n",
    "* **Hongyi Jin**: Equally\n",
    "* **Jingyu Ji**: Equally\n",
    "* **Jiaming Zhou**:Equally\n",
    "\n",
    "### 5.4 References\n",
    "[1] Nltk. Retrieved from https://www.geeksforgeeks.org/removing-stop-words-nltk-python/<br>\n",
    "[2] CountVectorizer. Retrieved from https://scikit-learn.org/stable/modules/feature_extraction.html <br>\n",
    "[3] Scikit-learn. Retrieved from https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
